{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16015625\n",
      "0.16574225122349107\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "mat = scipy.io.loadmat('spamData.mat')\n",
    "\n",
    "mat['Xtrain'] = np.log(mat['Xtrain'] + 0.1)\n",
    "mat['Xtest'] = np.log(mat['Xtest'] + 0.1)\n",
    "flattened_ytest = mat['ytest'].flatten() #This section is loading all the necessary data sets from matlab and manipulating some \n",
    "#data so they would be easier to deal with when executing the code\n",
    "\n",
    "##############################################################################################################################\n",
    "ytrain0 = []\n",
    "ytrain1 = []\n",
    "flattened_ytrain = mat['ytrain'].flatten()\n",
    "count = 0\n",
    "for i in range(3065):\n",
    "    if flattened_ytrain[i] == 1:\n",
    "        count += 1\n",
    "        ytrain1.append(mat['Xtrain'][i])\n",
    "    else:\n",
    "        ytrain0.append(mat['Xtrain'][i]) #split the ytrain datasets to ytrain datapoints that are 1(spam) and 0(non-spam)\n",
    "        \n",
    "L_ML = count / 3065\n",
    "ln_L_ML = np.log(L_ML)\n",
    "\n",
    "##############################################################################################################################\n",
    "neat_ytrain1 = np.array(ytrain1).transpose()\n",
    "neat_ytrain0 = np.array(ytrain0).transpose()\n",
    "\n",
    "mean_ytrain1 = [sum(item)/len(item) for item in neat_ytrain1] #computing the mean for class 1(spans across 57 features)\n",
    "mean_ytrain0 = [sum(item)/len(item) for item in neat_ytrain0] #computing the mean for class 0(spans across 57 features)\n",
    "\n",
    "before_summation = []#From this line onwards, it is the intermediate steps to computing the variance for the 2 classes\n",
    "before_summation0 = []\n",
    "for i in range(len(neat_ytrain1)):\n",
    "    before_summation_value = (neat_ytrain1[i] - mean_ytrain1[i])**2\n",
    "    before_summation.append(before_summation_value)\n",
    "    \n",
    "for i in range(len(neat_ytrain0)):\n",
    "    before_summation_value0 = (neat_ytrain0[i] - mean_ytrain0[i])**2\n",
    "    before_summation0.append(before_summation_value0)# up till this line\n",
    "\n",
    "variance_ytrain1 = [sum(item)/len(item) for item in before_summation] #computing the variance for class 1(spans across 57 features)\n",
    "variance_ytrain0 = [sum(item)/len(item) for item in before_summation0] #computing the variance for class 0(spans across 57 features)\n",
    "\n",
    "logpy1_list = []\n",
    "logpy0_list = []\n",
    "logpy1_list_train = []\n",
    "logpy0_list_train = []\n",
    "for item in mat['Xtest']:\n",
    "    logpy1 = sum(-(item - mean_ytrain1)**2/(2*np.array(variance_ytrain1)) - np.log(np.sqrt(2*np.pi*np.array(variance_ytrain1))))\\\n",
    "    + ln_L_ML\n",
    "    logpy1_list.append(logpy1) #computing the posterior for class 1(spam)\n",
    "    logpy0 = sum(-(item - mean_ytrain0)**2/(2*np.array(variance_ytrain0)) - np.log(np.sqrt(2*np.pi*np.array(variance_ytrain0))))\\\n",
    "    + np.log(1-L_ML)\n",
    "    logpy0_list.append(logpy0) #computing the posterior class 0(non spam)\n",
    "    \n",
    "correct = 0\n",
    "for i in range(1536):\n",
    "    if (np.array(logpy1_list)>np.array(logpy0_list)).astype(int)[i] == flattened_ytest[i]:\n",
    "        correct += 1 #comparing the 2 posteriors coming from 2 different classes and doing the prediction before checking it with \n",
    "        #ytest to compute the error rate\n",
    "\n",
    "        #The above section is trying to compute the test error rate\n",
    "#################################################################################################################################\n",
    "        #The below section is trying to compute the training error rate. Note that the computation steps below are generally \n",
    "        #similar to the ones above\n",
    "for item in mat['Xtrain']:\n",
    "    logpy1 = sum(-(item - mean_ytrain1)**2/(2*np.array(variance_ytrain1)) - np.log(np.sqrt(2*np.pi*np.array(variance_ytrain1))))\\\n",
    "    + ln_L_ML\n",
    "    logpy1_list_train.append(logpy1)\n",
    "    logpy0 = sum(-(item - mean_ytrain0)**2/(2*np.array(variance_ytrain0)) - np.log(np.sqrt(2*np.pi*np.array(variance_ytrain0))))\\\n",
    "    + np.log(1-L_ML)\n",
    "    logpy0_list_train.append(logpy0)\n",
    "    \n",
    "correct_train = 0\n",
    "for i in range(3065):\n",
    "    if (np.array(logpy1_list_train)>np.array(logpy0_list_train)).astype(int)[i] == flattened_ytrain[i]:\n",
    "        correct_train += 1\n",
    "\n",
    "print(1-correct/1536) #printing the test error rate\n",
    "print(1-correct_train/3065) #printing the training error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
